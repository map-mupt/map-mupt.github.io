---
layout: default
---

<article class="page">

  <h1>{{ page.title }}</h1>

  <div class="entry">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-justified">
        <p>
          While Large Language Models (LLMs) demonstrate impressive capabilities in text generation,
          we find that their ability has yet to be generalized to music, humanityâ€™s creative language.
          We introduce <b>ChatMusician</b>, an open-source LLM that integrates intrinsic musical
          abilities.
          It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music
          representation, ABC notation, and the music is treated as a second language.
          ChatMusician can understand and generate music with a pure text tokenizer without any
          external multi-modal neural structures or tokenizers.
          Interestingly, endowing musical abilities does not harm language abilities, even achieving a
          slightly higher MMLU score.
          Our model is capable of composing well-structured, full-length music, conditioned on texts,
          chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline.
          On our meticulously curated college-level music understanding benchmark,
          <b>MusicTheoryBench</b>, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a
          noticeable margin.
          Our work reveals that LLMs can be an excellent compressor for music, but there remains
          significant territory to be conquered.
          We release our 4B token music-language corpora <b>MusicPile</b>, the collected
          MusicTheoryBench, code, model and demo here.
        </p>
      </div>
    </div>
  </div>
</article>
